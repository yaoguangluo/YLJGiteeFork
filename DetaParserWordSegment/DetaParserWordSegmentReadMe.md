# 函数名
DNA元基催化:德塔极速分词包

## 使用须知

#### 引用
中华人民共和国 国家版权局 软著登字第3951366号 德塔自然语言图灵系统 V10.6.1  
CNN思想 提出者 Yann LeCun，Wei Zhang，Alexander Waibel 等   
ANN思想 数据挖掘教材   
RNN思想 提出者 M. I. Jordan，Jeffrey Elman   
HMM思想 提出者 隐.马尔可夫        
增加注释下: 我的6万中英文语义词库因为 有涉及 新华字典的词语录入. 没有在 德塔自然语言图灵系统 V10.6.1 个人软著的申请资源中.   
1 用复旦大学的免费分词软件(LGPL-3.0 license)进行将词语的词性标注.       
2 用百度,谷歌,有道,免费在线翻译进行词语的翻译, 特别感谢有道, 翻译质量最高.   
3 6万词库有2万是 百度上买了新华字典电子书录词语, 感谢新华字典. (仅仅录了2万个词语的名而已. 词语解释, 词语造句, 偏旁部首, 同义反义词汇等全部没有抄录.
我当时用浏览器中 英文单词对应的中文翻译记录的同义词汇)  
4 另外4万是我在新华字典的分词后 出现的未知词汇不断的记录统计下, 然后增加的.   
4.1 Github竟然还是卡阿卡的, 我就再细节溯源.我在百度文库上不但买了新华字典的词语, 我还买了成语词汇列表和国家中文语言过级的词汇,我都只录了词语而已.
4.1.1 再说一个细节, 记得当时我买了有一个词汇txt竟然是一行显示,我还因此把算法按行读改成了按字节buffer读. 可以查嘛.
4.2 我的购买方式是微信类扫码百度文库包月能免费下载买的.(我的支付宝虽然一年多没用了, 现在一直没有注销.国家可查询)
5 一部分词语是未知的单字进行同词性连着进行合并出来的新词并记录的.    
    
###### 申明
GNU GPL 2.0 协议 100% 源码全部开源发布, 避嫌.
申明下
1 我的分词方法函数在2019年4月3号就写完了, 2019年5月28号就下证了.
2 复旦大学的邹锡鹏 在同年7月才提出新的分词方法.
3 我的源码已经全部开源, 双方git都有开源, giff一下, 是否逻辑相同.就是了,新的分词方法和我相同就是 邹锡鹏抄袭.
4 我的源码4月3号的 和 复旦的逻辑相同 就是 罗瑶光抄袭. 简单的很. 注明我的分词当时每秒达到1800万中文分词速度.
当时 lucene 中文分词速度每秒20万.

罗瑶光
20210507